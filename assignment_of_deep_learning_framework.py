# -*- coding: utf-8 -*-
"""ASSIGNMENT OF DEEP LEARNING FRAMEWORK

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HzbF4rNn0hjHtSPHqNidoUm8S6Dhw1wZ

Assignmnet of Deep Learning Frameworks

Q1. What is TensorFlow 2.0, and how is it different from TensorFlow 1.x2

TensorFlow 2.0 is a machine learning and deep learning framework used to build, train, and deploy ML/DL models. It emphasizes:

Ease of use

Eager execution

High-level APIs (Keras as default)

Better performance and debugging

| Feature                | TensorFlow 1.x                         | TensorFlow 2.0                        |
| ---------------------- | -------------------------------------- | ------------------------------------- |
| **Execution Mode**     | Graph-based (static computation graph) | Eager execution (default)             |
| **Code Style**         | Complex and verbose                    | Simple, Pythonic                      |
| **Keras Integration**  | Optional (`tf.keras`)                  | Fully integrated and default          |
| **Session Usage**      | Requires `tf.Session()`                | No sessions needed                    |
| **Placeholders**       | Uses `tf.placeholder()`                | Not used                              |
| **Debugging**          | Difficult (graph execution)            | Easy (eager execution)                |
| **API Consistency**    | Multiple redundant APIs                | Clean and unified APIs                |
| **Model Building**     | Low-level APIs common                  | High-level APIs encouraged            |
| **Production Support** | Limited                                | Improved (TF Serving, TF Lite, TF.js) |

Q.2.How do you install TensorFlow 2.02

Steps to install TensorFlow 2.0.2:

Install Python 3.6 / 3.7

Create & activate virtual environment

Upgrade pip

Run pip install tensorflow==2.0.2

Verify installation using tf.__version__
"""

# tensorflow_test.py
# Works for TensorFlow 2.0.2

import tensorflow as tf

def main():
    print("TensorFlow Version:", tf.__version__)

    # Basic tensor operations
    a = tf.constant(10)
    b = tf.constant(20)

    c = a + b

    print("Tensor a:", a)
    print("Tensor b:", b)
    print("Result (a + b):", c)

    # Simple computation
    x = tf.constant([1, 2, 3, 4])
    y = tf.constant([5, 6, 7, 8])

    z = tf.add(x, y)
    print("Vector Addition:", z)

main()

"""Q.3. What is the primary function of the tf.function in TensorFlow 2.02

The primary function of tf.function in TensorFlow 2.0.2 is to convert a regular Python function into a TensorFlow computation graph for faster execution and better performance.

TensorFlow 2.x runs in eager execution mode by default (easy to debug but slower).

tf.function wraps a Python function and compiles it into a static graph.

This allows TensorFlow to apply graph optimizations, resulting in:

Faster execution

Better performance

Easier deployment (production-ready models)
"""

#example of TF Function
import tensorflow as tf

@tf.function
def multiply(a, b):
    return a * b

result = multiply(3, 4)
print(result)

"""Q.8. What is the purpose of the Model class in TensorFlow 2.02

Purpose of the Model Class

The tf.keras.Model class provides:

A high-level abstraction for neural networks

Built-in support for training, evaluation, and prediction

Easy model customization and reuse

Integration with TensorFlow‚Äôs optimizers, losses, and metrics

Key Functions Provided

.. compile() ‚Äì configure loss, optimizer, metrics
.. fit() ‚Äì train the model
.. evaluate() ‚Äì test the model
.. predict() ‚Äì make predictions
.. save() / load_model() ‚Äì model persistence
"""

import tensorflow as tf

class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(10, activation='relu')
        self.dense2 = tf.keras.layers.Dense(1)

    def call(self, inputs):
        x = self.dense1(inputs)
        return self.dense2(x)

"""Q.5. How do you create a neural network using TensorFlow 2.02

Steps to Create a Neural Network in TensorFlow 2.0.2

Import TensorFlow

Define the model (using Sequential or Model class)

Add layers

Compile the model

Train the model

Evaluate / Predict
"""

# neural_network_tf2.py
# Compatible with TensorFlow 2.0.2

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np

def main():
    # 1. Create dummy training data
    X = np.array([[1], [2], [3], [4]], dtype=float)
    y = np.array([[2], [4], [6], [8]], dtype=float)

    # 2. Define the neural network
    model = Sequential([
        Dense(8, activation='relu', input_shape=(1,)),
        Dense(1)
    ])

    # 3. Compile the model
    model.compile(
        optimizer='adam',
        loss='mse',
        metrics=['mae']
    )

    # 4. Train the model
    model.fit(X, y, epochs=200, verbose=0)

    # 5. Evaluate the model
    loss, mae = model.evaluate(X, y, verbose=0)
    print("Loss:", loss)
    print("MAE:", mae)

    # 6. Make predictions
    prediction = model.predict(np.array([[5]], dtype=float))
    print("Prediction for input 5:", prediction)

if __name__ == "__main__":
    main()

"""Q.6. What is the importance of Tensor Space in TensorFlow2

Tensor Space in TensorFlow 2 refers to the multi-dimensional space in which tensors (data) exist and operate. It defines how data is represented, stored, and manipulated during computation in deep learning models.

Importance of Tensor Space in TensorFlow 2
1. Core Data Structure

Tensors are the basic building blocks of TensorFlow.

All inputs, weights, outputs, and gradients are represented as tensors.

Without tensor space, no computation is possible.

2. Supports Multi-Dimensional Data

Tensor space allows handling of:

Scalars (0D) ‚Äì single value

Vectors (1D) ‚Äì list of values

Matrices (2D) ‚Äì tables

Higher-order tensors (3D, 4D, ‚Ä¶) ‚Äì images, videos, sequences

Q.7. How can TensorBoard be integrated with TensorFlow 2.02
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import TensorBoard
import numpy as np
import datetime

# Data
X = np.array([[1], [2], [3], [4]], dtype=float)
y = np.array([[2], [4], [6], [8]], dtype=float)

# Model
model = Sequential([
    Dense(8, activation='relu', input_shape=(1,)),
    Dense(1)
])

# Compile
model.compile(optimizer='adam', loss='mse')

# TensorBoard setup
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

# Train
model.fit(X, y, epochs=20, callbacks=[tensorboard_callback])

"""Q.8  What is the purpose of TensorFlow Playground2

Purpose of TensorFlow Playground
1. Learning & Visualization

Visualizes neurons, layers, weights, and activations

Shows how data flows through a neural network

Displays real-time changes during training

2. Understanding Neural Network Concepts

It helps in learning:

Effect of number of layers & neurons

Role of activation functions

Impact of learning rate

Overfitting vs underfitting

Decision boundaries

3. Experimentation Without Coding

Users can change parameters using sliders

No need to write TensorFlow or Python code

Ideal for beginners and students

4. Demonstrating Training Behavior

Shows how loss decreases

Explains backpropagation intuitively

Helps understand feature transformation

5. Teaching & Academic Use

Widely used in classrooms, labs, and presentations

Useful for exams, viva, and demos

Q.9.  What is Netron, and how is it useful for deep learning models2?

Netron is a model visualization tool used to inspect, analyze, and understand deep learning models by displaying their network architecture in a graphical form.

Netron is a model visualization tool used to inspect, analyze, and understand deep learning models by displaying their network architecture in a graphical form.

How Netron is Useful in Deep Learning

1. Visualizing Model Architecture

Displays layers, connections, inputs, and outputs

Helps understand complex neural networks clearly

2. Model Debugging

Detects incorrect layer connections

Identifies shape mismatches

Useful when a model gives unexpected results

3. Model Verification

Confirms whether the saved model matches the intended architecture

Useful before deployment or sharing models

4. Framework-Independent Inspection

Allows viewing models from different frameworks in one tool

Helpful in model conversion (e.g., TensorFlow ‚Üí ONNX)

5. Educational & Documentation Use

Helps students learn model structure visually

Used in reports, presentations, and exams

Q10. What is the difference between TensorFlow and PyTorch2

TensorFlow and PyTorch are two widely used deep learning frameworks, each designed with different goals and philosophies. TensorFlow, developed by Google, was originally based on a static computation graph approach, where the model is defined first and then executed. In TensorFlow 2.x, eager execution and the Keras API have made it more user-friendly and suitable for large-scale production systems. TensorFlow is well known for its strong support for deployment on servers, mobile devices, and web platforms.

On the other hand, PyTorch, developed by Facebook (Meta), uses a dynamic computation graph by default. This allows the graph to be built and modified during runtime, making PyTorch highly flexible and intuitive. Because it closely follows standard Python programming, it is easier to debug and is widely preferred in research and academic environments.

In terms of usage, TensorFlow is mainly favored in industry and production-level applications due to its scalability and deployment tools, while PyTorch is commonly used in research and experimentation because of its simplicity and flexibility. Both frameworks support GPU acceleration, automatic differentiation, and modern neural network architectures, but they differ mainly in execution style, ease of debugging, and deployment capabilities.

Q.11 How do you install PyTorch2
"""

pip install pytorch2

"""A PyTorch neural network is built in a modular and object-oriented manner using the torch.nn module. The basic structure consists of defining the model, specifying the forward pass, choosing a loss function, selecting an optimizer, and training the model.

1. Import Required Modules

The first step is to import PyTorch and its neural network utilities. The torch module provides tensor operations, while torch.nn supplies predefined layers and loss functions.

2. Define the Neural Network Class

A neural network in PyTorch is defined by creating a class that inherits from nn.Module. Inside the class:

The __init__() method defines layers such as fully connected, convolutional, or recurrent layers.

The forward() method defines how input data flows through the network.

3. Forward Pass

The forward pass describes the sequence of operations applied to the input data to produce the output. PyTorch automatically constructs the computation graph during this step.

4. Loss Function

A loss function measures the difference between the predicted output and the true target values. Common examples include Mean Squared Error and Cross-Entropy Loss.

5. Optimizer

The optimizer updates the model‚Äôs parameters based on gradients computed during backpropagation. Examples include SGD, Adam, and RMSprop.

6. Training Loop

The training process involves:

Forward pass

Loss computation

Backward pass (loss.backward())

Parameter update (optimizer.step())

Conceptual Flow

Input ‚Üí Layers ‚Üí Activation Functions ‚Üí Output ‚Üí Loss ‚Üí Backpropagation ‚Üí Weight Update

## Q.12. What is the basic structure of a PyTorch neural network2

Input ‚Üí Layers ‚Üí Activations ‚Üí Output ‚Üí Loss ‚Üí Backpropagation ‚Üí Weight Update
"""

# basic_pytorch_nn.py
# PyTorch basic neural network example

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# 1. Prepare training data (simple linear relationship y = 2x)
X = np.array([[1], [2], [3], [4]], dtype=np.float32)
y = np.array([[2], [4], [6], [8]], dtype=np.float32)

# Convert numpy arrays to PyTorch tensors
X_train = torch.from_numpy(X)
y_train = torch.from_numpy(y)

# 2. Define the neural network model
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(1, 8)  # 1 input, 8 hidden neurons
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(8, 1)  # 1 output

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# Instantiate the model
model = SimpleNN()

# 3. Define loss function and optimizer
criterion = nn.MSELoss()           # Mean Squared Error for regression
optimizer = optim.Adam(model.parameters(), lr=0.01)

# 4. Training loop
epochs = 200
for epoch in range(epochs):
    # Forward pass
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch+1) % 20 == 0:
        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')

# 5. Test the model
with torch.no_grad():
    test_input = torch.tensor([[5]], dtype=torch.float32)
    prediction = model(test_input)
    print(f'Prediction for input 5: {prediction.item():.4f}')

"""Q13. What is the significance of tensors in PyTorch2

Significance of Tensors in PyTorch

Tensors are the core data structure in PyTorch, similar to NumPy arrays, but with additional capabilities for deep learning and GPU acceleration. They are fundamental because all computations in PyTorch, including neural network operations, are performed on tensors.

Key Points: Why Tensors Are Important in PyTorch
1. Core Building Block

All inputs, outputs, model parameters (weights, biases), and intermediate data are represented as tensors.

Every neural network computation uses tensors.

2. Multi-Dimensional Data Representation

Tensors can represent scalars (0D), vectors (1D), matrices (2D), and higher-dimensional data (3D, 4D, ‚Ä¶).

Useful for images, videos, sequences, and batches.
"""

import torch
x = torch.tensor([[1, 2], [3, 4]])  # 2D tensor

import torch
x = torch.tensor([[1, 2], [3, 4]])  # 2D tensor

"""Q14. What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch

| Feature                  | `torch.Tensor`                    | `torch.cuda.Tensor`                                                                  |
| ------------------------ | --------------------------------- | ------------------------------------------------------------------------------------ |
| Default device           | CPU                               | GPU (CUDA)                                                                           |
| Memory location          | System RAM                        | GPU VRAM                                                                             |
| Speed                    | Slower for large-scale operations | Much faster for parallel computation                                                 |
| Creation syntax          | `torch.Tensor(...)`               | `torch.Tensor(...).cuda()` or `torch.cuda.FloatTensor(...)`                          |
| Requires GPU?            | No                                | Yes                                                                                  |
| Operations compatibility | CPU operations                    | GPU operations only (mixing CPU and GPU tensors requires `.to(device)` or `.cuda()`) |

Q15. What is the purpose of the torch.optim module in PyTorch2

Purpose of torch.optim

Optimize Model Parameters

Neural networks learn by adjusting their weights to minimize a loss function.

torch.optim contains optimization algorithms (optimizers) that perform this weight update automatically.

Implement Gradient-Based Updates

PyTorch computes gradients using autograd (loss.backward()), but it does not update the weights automatically.

Optimizers in torch.optim use these gradients to update the model parameters according to specific rules (like SGD, Adam, RMSProp).

Support for Many Optimizers

Examples of optimizers:

SGD (Stochastic Gradient Descent) ‚Äì simple gradient descent with optional momentum.

Adam ‚Äì adaptive learning rates, very popular for deep learning.

RMSprop, Adagrad, Adadelta ‚Äì other variants for specific cases.

Learning Rate Scheduling

Many optimizers allow learning rate adjustments dynamically for better convergence.
"""

import torch
import torch.nn as nn
import torch.optim as optim

# Example model
model = nn.Linear(10, 1)  # simple linear layer

# Loss function
criterion = nn.MSELoss()

# Optimizer
optimizer = optim.SGD(model.parameters(), lr=0.01)  # using SGD

# Dummy input and target
x = torch.randn(5, 10)
y = torch.randn(5, 1)

# Forward pass
output = model(x)
loss = criterion(output, y)

# Backward pass
loss.backward()  # compute gradients

# Update weights
optimizer.step()  # apply optimizer to adjust weights

# Zero gradients for next iteration
optimizer.zero_grad()

"""Q.16 What are some common activation functions used in neural networks2

1. Sigmoid

Formula:

ùúé
(
ùë•
)
=
1
1
+
ùëí
‚àí
ùë•
œÉ(x)=
1+e
‚àíx
1
	‚Äã


Range: 0 to 1

Use: Often used in binary classification (output layer).

Pros: Smooth, differentiable, outputs probabilities.

Cons: Can cause vanishing gradient for very large/small inputs.

2. Tanh (Hyperbolic Tangent)

Formula:

tanh
‚Å°
(
ùë•
)
=
ùëí
ùë•
‚àí
ùëí
‚àí
ùë•
ùëí
ùë•
+
ùëí
‚àí
ùë•
tanh(x)=
e
x
+e
‚àíx
e
x
‚àíe
‚àíx
	‚Äã


Range: -1 to 1

Use: Hidden layers often use Tanh to center data around 0.

Pros: Zero-centered, stronger gradients than sigmoid.

Cons: Still suffers from vanishing gradient for very large/small inputs.

3. ReLU (Rectified Linear Unit)

Formula:

ùëì
(
ùë•
)
=
max
‚Å°
(
0
,
ùë•
)
f(x)=max(0,x)

Range: 0 to ‚àû

Use: Most popular for hidden layers in deep networks.

Pros: Computationally efficient, avoids vanishing gradient for positive inputs.

Cons: Can cause ‚Äúdying ReLU‚Äù problem (neurons stuck at 0).

4. Leaky ReLU

Formula:

ùëì
(
ùë•
)
=
{
ùë•

ùë•
>
0


ùõº
ùë•

ùë•
‚â§
0
f(x)={
x
Œ±x
	‚Äã

x>0
x‚â§0
	‚Äã


Range: -‚àû to ‚àû

Use: Hidden layers when you want to avoid dying ReLU.

Pros: Small gradient for negative inputs (usually Œ± = 0.01).

5. Softmax

Formula:

ùúé
(
ùëß
ùëñ
)
=
ùëí
ùëß
ùëñ
‚àë
ùëó
ùëí
ùëß
ùëó
œÉ(z
i
	‚Äã

)=
‚àë
j
	‚Äã

e
z
j
	‚Äã

e
z
i
	‚Äã

	‚Äã


Range: 0 to 1 (all outputs sum to 1)

Use: Multi-class classification output layer.

Pros: Converts raw scores into probabilities.

6. Swish (Modern Alternative)

Formula:

f(x)=x‚ãÖœÉ(x)

Range: -0.28 to ‚àû approximately

Use: Hidden layers in modern deep networks (like transformers).

Pros: Smooth, avoids dying neurons, sometimes gives better accuracy than ReLU.

Q.17. What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch2

1. torch.nn.Module

Definition:
Module is the base class for all neural network models in PyTorch.

Flexibility: Very flexible ‚Äì you can define any kind of architecture, including branching, skip connections, multiple inputs/outputs, or custom forward passes.

Key Requirement: You must define a forward() method that specifies how input flows through the network.
"""

import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.fc1 = nn.Linear(10, 20)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(20, 1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

model = MyModel()
print(model)

"""2. torch.nn.Sequential

Definition:
Sequential is a special Module for stacking layers in a simple, linear sequence.

Flexibility: Less flexible than Module because you can only define straightforward, sequential architectures.
"""

import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 1)
)

print(model)

"""Q.18.How can you monitor training progress in TensorFlow 2.02

1. Using History Object

When you call model.fit(), TensorFlow returns a History object containing loss and metric values for each epoch.
"""

import tensorflow as tf
from tensorflow.keras import layers, models


model = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(10,)),
    layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

import numpy as np
x_train = np.random.rand(100, 10)
y_train = np.random.rand(100, 1)

history = model.fit(x_train, y_train, epochs=10, validation_split=0.2)

print(history.history['loss'])
print(history.history['val_loss'])

"""2. Plotting Loss & Accuracy Curves"""

import matplotlib.pyplot as plt

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""3.Using TensorBoard"""

import tensorflow as tf
from tensorflow.keras.callbacks import TensorBoard
import datetime

log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)

model.fit(
    x_train, y_train,
    epochs=10,
    validation_split=0.2,
    callbacks=[tensorboard_callback]
)

"""Q.19. How does the Keras API fit into TensorFlow 2.02

Keras is a high-level neural network API that makes it easy to define and train models.

In TensorFlow 2.x, Keras is available as tf.keras.

It provides user-friendly abstractions over lower-level TensorFlow operations like tensors, gradients, and optimizers.


2. Key Features of tf.keras

Model building

Supports Sequential API for simple linear stacks of layers.

Supports Functional API for complex architectures (branching, multiple inputs/outputs).

Supports subclassing tf.keras.Model for fully custom models.

Layers and modules

Provides standard layers: Dense, Conv2D, LSTM, BatchNormalization, etc.

Training and evaluation

model.compile() ‚Äì specify loss, optimizer, and metrics.

model.fit() ‚Äì train your model on data.

model.evaluate() ‚Äì test model performance.

model.predict() ‚Äì make predictions.

Callbacks

Easily integrate monitoring tools like TensorBoard, EarlyStopping, and ModelCheckpoint.

Integration with TensorFlow ecosystem

Works seamlessly with TF datasets, TF data pipelines, TF functions, and GPU/TPU acceleration.
"""

import tensorflow as tf
from tensorflow.keras import layers, models

# Simple Sequential model
model = models.Sequential([
    layers.Dense(64, activation='relu', input_shape=(10,)),
    layers.Dense(1)
])

# Compile model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train model
import numpy as np
x_train = np.random.rand(100, 10)
y_train = np.random.rand(100, 1)
history = model.fit(x_train, y_train, epochs=10, validation_split=0.2)

x_train
y_train

"""Q.20 What is an example of a deep learning project that can be implemented using TensorFlow 2.02

1. Project Overview
2. Implementation Steps in TensorFlow 2.x
3: Build the CNN Model
4: Compile the Model
5: Train the Model
6: Evaluate the Model
7: Visualize Training

Q.21What is the main advantage of using pre-trained models in TensorFlow and PyTorch?

1. What is a Pre-trained Model?

A pre-trained model is a neural network that has already been trained on a large dataset (e.g., ImageNet for images, or WikiText for language models).

Instead of training from scratch, you can reuse the weights of the network and either:

Fine-tune it on your own dataset (transfer learning).

Use it as a feature extractor (freeze most layers and only train a few final layers).
"""

